{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6966f45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Computer vision imports\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.retinanet import RetinaNetHead\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "# Image processing imports\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c55870f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Unified Configuration ---\n",
    "current_dir = Path.cwd()\n",
    "project_root = current_dir.parents[1]\n",
    "\n",
    "CONFIG = {\n",
    "    # --- Dataset and Environment ---\n",
    "    \"DATA_DIR\": project_root / \"data\" / \"object_detection_dataset\",\n",
    "    \"IMAGES_DIR\": \"images\",\n",
    "    \"ANNOTATION_FILE\": \"_annotations.coco.json\",\n",
    "    \"RESULTS_DIR\": \"research_results\",\n",
    "    \"DEVICE\": \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\",\n",
    "    \n",
    "    # --- Evaluation Parameters ---\n",
    "    \"K_SHOTS_TO_TEST\": [1, 2, 3, 5, 10],\n",
    "    \"N_EVAL_EPISODES\": 30,\n",
    "    \n",
    "    # --- Fine-Tuning Parameters ---\n",
    "    \"FT_EPOCHS\": 15,\n",
    "    \"FT_LEARNING_RATE\": 0.001,\n",
    "    \"FT_BATCH_SIZE\": 1,\n",
    "\n",
    "    # --- Learning Rate Scheduler Parameters ---\n",
    "    \"LR_SCHEDULER_STEP_SIZE\": 5,\n",
    "    \"LR_SCHEDULER_GAMMA\": 0.5,\n",
    "\n",
    "    # --- Visualization ---\n",
    "    \"VISUALIZE_SAMPLES\": 30,\n",
    "    \"CONFIDENCE_THRESHOLD\": 0.5\n",
    "}\n",
    "\n",
    "# Experiment definitions (simplified)\n",
    "EXPERIMENTS = [\n",
    "    {\"name\": \"TFA_fc_FasterRCNN\", \"freeze\": \"backbone\", \"classifier\": \"fc\"},\n",
    "    {\"name\": \"TFA_cos_FasterRCNN\", \"freeze\": \"backbone\", \"classifier\": \"cos\", \"scale\": 20.0},\n",
    "    {\"name\": \"Full_Finetune_FasterRCNN\", \"freeze\": \"none\", \"classifier\": \"fc\"},\n",
    "    {\"name\": \"TFA_fc_Augmented\", \"freeze\": \"backbone\", \"classifier\": \"fc\", \"augment\": True}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a0519ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Simplified Data Handling ---\n",
    "def get_transforms(use_augmentation=False):\n",
    "    \"\"\"Simple transform factory.\"\"\"\n",
    "    if use_augmentation:\n",
    "        return A.Compose([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.ColorJitter(brightness=0.2, contrast=0.2, p=0.5),\n",
    "            ToTensorV2(),\n",
    "        ], bbox_params=A.BboxParams(format='coco', label_fields=['labels']))\n",
    "    return A.Compose([ToTensorV2()], bbox_params=A.BboxParams(format='coco', label_fields=['labels']))\n",
    "\n",
    "class SimpleCocoDataset(Dataset):\n",
    "    \"\"\"Simplified COCO dataset with essential functionality.\"\"\"\n",
    "    \n",
    "    def __init__(self, transforms=None):\n",
    "        self.root = os.path.join(CONFIG[\"DATA_DIR\"], CONFIG[\"IMAGES_DIR\"])\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        # Load COCO data\n",
    "        with open(os.path.join(CONFIG[\"DATA_DIR\"], CONFIG[\"ANNOTATION_FILE\"])) as f:\n",
    "            coco_data = json.load(f)\n",
    "        \n",
    "        self.images = coco_data['images']\n",
    "        self.annotations = coco_data['annotations']\n",
    "        self.categories = coco_data['categories']\n",
    "        \n",
    "        # Create mappings\n",
    "        self._build_mappings()\n",
    "        \n",
    "    def _build_mappings(self):\n",
    "        \"\"\"Build category and image mappings.\"\"\"\n",
    "        self.cat_id_to_name = {cat['id']: cat['name'] for cat in self.categories}\n",
    "        self.cat_id_to_label = {cat['id']: i + 1 for i, cat in enumerate(self.categories)}\n",
    "        self.label_to_name = {v: self.cat_id_to_name[k] for k, v in self.cat_id_to_label.items()}\n",
    "        \n",
    "        # Group annotations by image\n",
    "        self.img_to_anns = defaultdict(list)\n",
    "        self.cat_to_imgs = defaultdict(set)\n",
    "        for ann in self.annotations:\n",
    "            self.img_to_anns[ann['image_id']].append(ann)\n",
    "            self.cat_to_imgs[ann['category_id']].add(ann['image_id'])\n",
    "        \n",
    "        self.img_ids = [img['id'] for img in self.images]\n",
    "        self.id_to_idx = {img_id: i for i, img_id in enumerate(self.img_ids)}\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_info = self.images[idx]\n",
    "        img_path = os.path.join(self.root, img_info['file_name'])\n",
    "        img = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        \n",
    "        # Process annotations\n",
    "        anns = self.img_to_anns[img_info['id']]\n",
    "        boxes, labels = self._process_annotations(anns, img.shape[:2])\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transforms:\n",
    "            transformed = self.transforms(image=img, bboxes=boxes, labels=labels)\n",
    "            img_tensor = transformed['image'] / 255.0\n",
    "            boxes = self._convert_boxes(transformed['bboxes'])\n",
    "            labels = torch.tensor(transformed['labels'], dtype=torch.int64)\n",
    "        else:\n",
    "            img_tensor = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0\n",
    "            boxes = self._convert_boxes(boxes)\n",
    "            labels = torch.tensor(labels, dtype=torch.int64)\n",
    "            \n",
    "        target = {\"boxes\": boxes, \"labels\": labels}\n",
    "        original_img = Image.fromarray(img)\n",
    "        \n",
    "        return img_tensor, target, original_img\n",
    "    \n",
    "    def _process_annotations(self, anns, img_shape):\n",
    "        \"\"\"Extract valid boxes and labels from annotations.\"\"\"\n",
    "        boxes, labels = [], []\n",
    "        img_h, img_w = img_shape\n",
    "        \n",
    "        for ann in anns:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            # Clamp to image bounds\n",
    "            x_max, y_max = min(x + w, img_w), min(y + h, img_h)\n",
    "            if x_max > x and y_max > y:\n",
    "                boxes.append([x, y, x_max - x, y_max - y])\n",
    "                labels.append(self.cat_id_to_label[ann['category_id']])\n",
    "        \n",
    "        # Handle empty annotations\n",
    "        if not boxes:\n",
    "            return self.__getitem__(random.randint(0, len(self) - 1))[:2]\n",
    "            \n",
    "        return boxes, labels\n",
    "    \n",
    "    def _convert_boxes(self, boxes):\n",
    "        \"\"\"Convert COCO format to PyTorch format.\"\"\"\n",
    "        if not boxes:\n",
    "            return torch.zeros((0, 4), dtype=torch.float32)\n",
    "        boxes_tensor = torch.tensor(boxes, dtype=torch.float32)\n",
    "        # Convert from [x, y, w, h] to [x1, y1, x2, y2]\n",
    "        boxes_tensor[:, 2] = boxes_tensor[:, 0] + boxes_tensor[:, 2]\n",
    "        boxes_tensor[:, 3] = boxes_tensor[:, 1] + boxes_tensor[:, 3]\n",
    "        return boxes_tensor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "affc447a-b8d8-45ac-a579-adcb26057724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Simplified Model Components ---\n",
    "class CosineSimilarityClassifier(nn.Module):\n",
    "    \"\"\"Cosine similarity classifier for few-shot learning.\"\"\"\n",
    "    def __init__(self, in_features, out_features, scale=20.0):\n",
    "        super().__init__()\n",
    "        self.scale = scale\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        nn.init.kaiming_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.scale * F.linear(F.normalize(x), F.normalize(self.weight))\n",
    "\n",
    "class CosineFastRCNNPredictor(FastRCNNPredictor):\n",
    "    \"\"\"Fast R-CNN predictor with cosine similarity.\"\"\"\n",
    "    def __init__(self, in_channels, num_classes, scale=20.0):\n",
    "        super().__init__(in_channels, num_classes)\n",
    "        self.cls_score = CosineSimilarityClassifier(in_channels, num_classes, scale)\n",
    "\n",
    "def create_model(num_classes, exp_config):\n",
    "    \"\"\"Create and configure model based on experiment config.\"\"\"\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(weights='DEFAULT')\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    num_classes_with_bg = num_classes + 1\n",
    "    \n",
    "    # Configure classifier\n",
    "    if exp_config.get('classifier') == 'cos':\n",
    "        scale = exp_config.get('scale', 20.0)\n",
    "        model.roi_heads.box_predictor = CosineFastRCNNPredictor(in_features, num_classes_with_bg, scale)\n",
    "    else:\n",
    "        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes_with_bg)\n",
    "    \n",
    "    # Apply freezing strategy\n",
    "    if exp_config.get('freeze') == 'backbone':\n",
    "        for param in model.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        print(\"Backbone frozen\")\n",
    "    else:\n",
    "        print(\"Model fully trainable\")\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7629e9cd-500f-4b7e-8115-0d75fa5a9506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Simplified Helper Functions ---\n",
    "def create_episode_split(dataset, k_shot):\n",
    "    \"\"\"Create support and query splits for few-shot learning.\"\"\"\n",
    "    support_img_ids = set()\n",
    "    \n",
    "    # Sample k images per category\n",
    "    for cat_id in dataset.cat_to_imgs.keys():\n",
    "        img_ids = list(dataset.cat_to_imgs[cat_id])\n",
    "        random.shuffle(img_ids)\n",
    "        support_img_ids.update(img_ids[:min(k_shot, len(img_ids))])\n",
    "    \n",
    "    # Create indices\n",
    "    support_indices = [dataset.id_to_idx[img_id] for img_id in support_img_ids]\n",
    "    query_indices = [i for i, img_id in enumerate(dataset.img_ids) if img_id not in support_img_ids]\n",
    "    \n",
    "    return (torch.utils.data.Subset(dataset, support_indices), \n",
    "            torch.utils.data.Subset(dataset, query_indices))\n",
    "\n",
    "def evaluate_model(model, query_loader):\n",
    "    \"\"\"Evaluate model and return mAP metrics.\"\"\"\n",
    "    model.eval()\n",
    "    metric = MeanAveragePrecision(box_format='xyxy').to(CONFIG['DEVICE'])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets, _ in query_loader:\n",
    "            images = [img.to(CONFIG['DEVICE']) for img in images]\n",
    "            targets = [{k: v.to(CONFIG['DEVICE']) for k, v in t.items()} for t in targets]\n",
    "            predictions = model(images)\n",
    "            metric.update(predictions, targets)\n",
    "    \n",
    "    results = metric.compute()\n",
    "    return {\n",
    "        'map': results['map'].item(),\n",
    "        'map_50': results['map_50'].item(), \n",
    "        'map_75': results['map_75'].item()\n",
    "    }\n",
    "\n",
    "def train_model(model, support_loader, exp_config):\n",
    "    \"\"\"Train model on support set.\"\"\"\n",
    "    # Setup optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=CONFIG[\"FT_LEARNING_RATE\"], \n",
    "                               momentum=0.9, weight_decay=0.0005)\n",
    "    \n",
    "    # Setup scheduler if needed\n",
    "    scheduler = None\n",
    "    if exp_config.get(\"use_scheduler\"):\n",
    "        scheduler = StepLR(optimizer, step_size=CONFIG[\"LR_SCHEDULER_STEP_SIZE\"], \n",
    "                          gamma=CONFIG[\"LR_SCHEDULER_GAMMA\"])\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    if exp_config.get('freeze') == 'backbone':\n",
    "        model.backbone.eval()\n",
    "    \n",
    "    for epoch in range(CONFIG[\"FT_EPOCHS\"]):\n",
    "        for images, targets, _ in support_loader:\n",
    "            images = [img.to(CONFIG['DEVICE']) for img in images]\n",
    "            targets = [{k: v.to(CONFIG['DEVICE']) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "def run_single_experiment(exp_config, dataset):\n",
    "    \"\"\"Run a complete experiment for one configuration.\"\"\"\n",
    "    exp_name = exp_config['name']\n",
    "    results_dir = f\"{CONFIG['RESULTS_DIR']}/{exp_name}\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\\nRUNNING: {exp_name}\\n{'='*50}\")\n",
    "    \n",
    "    # Configure dataset transforms\n",
    "    use_augment = exp_config.get('augment', False)\n",
    "    dataset.transforms = get_transforms(use_augment)\n",
    "    \n",
    "    results = defaultdict(list)\n",
    "    num_classes = len(dataset.categories)\n",
    "    \n",
    "    for k in CONFIG[\"K_SHOTS_TO_TEST\"]:\n",
    "        print(f\"\\n--- K={k} shots ---\")\n",
    "        episode_results = defaultdict(list)\n",
    "        \n",
    "        for episode in tqdm(range(CONFIG[\"N_EVAL_EPISODES\"]), desc=f\"K={k}\"):\n",
    "            # Create model and splits\n",
    "            model = create_model(num_classes, exp_config).to(CONFIG['DEVICE'])\n",
    "            support_set, query_set = create_episode_split(dataset, k)\n",
    "            \n",
    "            if not support_set or not query_set:\n",
    "                continue\n",
    "                \n",
    "            # Train and evaluate\n",
    "            support_loader = DataLoader(support_set, batch_size=CONFIG[\"FT_BATCH_SIZE\"], \n",
    "                                      shuffle=True, collate_fn=collate_fn)\n",
    "            query_loader = DataLoader(query_set, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "            \n",
    "            train_model(model, support_loader, exp_config)\n",
    "            metrics = evaluate_model(model, query_loader)\n",
    "            \n",
    "            for metric, value in metrics.items():\n",
    "                episode_results[metric].append(value)\n",
    "        \n",
    "        # Aggregate results\n",
    "        if episode_results['map']:\n",
    "            results['k'].append(k)\n",
    "            for metric in ['map', 'map_50', 'map_75']:\n",
    "                values = episode_results[metric]\n",
    "                results[f'{metric}_mean'].append(np.mean(values))\n",
    "                results[f'{metric}_std'].append(np.std(values))\n",
    "    \n",
    "    # Save results\n",
    "    with open(f\"{results_dir}/results.json\", 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973c037f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 298 images, 7 classes\n",
      "Device: mps\n",
      "\n",
      "==================================================\n",
      "RUNNING: TFA_fc_FasterRCNN\n",
      "==================================================\n",
      "\n",
      "--- K=1 shots ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K=1:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone frozen\n"
     ]
    }
   ],
   "source": [
    "# --- Simplified Main Execution ---\n",
    "def plot_results(results_dir, experiments):\n",
    "    \"\"\"Generate comparison plots for all metrics.\"\"\"\n",
    "    metrics = {'map': 'mAP (IoU .50-.95)', 'map_50': 'mAP@.50', 'map_75': 'mAP@.75'}\n",
    "    \n",
    "    for metric, display_name in metrics.items():\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        for exp in experiments:\n",
    "            results_file = f\"{results_dir}/{exp['name']}/results.json\"\n",
    "            if os.path.exists(results_file):\n",
    "                with open(results_file) as f:\n",
    "                    data = json.load(f)\n",
    "                if f'{metric}_mean' in data:\n",
    "                    plt.errorbar(data['k'], data[f'{metric}_mean'], yerr=data[f'{metric}_std'],\n",
    "                               fmt='-o', capsize=5, label=exp['name'], markersize=8)\n",
    "        \n",
    "        plt.title(f'Few-Shot Object Detection Comparison - {display_name}')\n",
    "        plt.xlabel('Number of Support Shots (K)')\n",
    "        plt.ylabel(display_name)\n",
    "        plt.xticks(CONFIG[\"K_SHOTS_TO_TEST\"])\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{results_dir}/comparison_{metric}.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    # Setup\n",
    "    results_dir = CONFIG[\"RESULTS_DIR\"]\n",
    "    if os.path.exists(results_dir):\n",
    "        shutil.rmtree(results_dir)\n",
    "    os.makedirs(results_dir)\n",
    "    \n",
    "    # Load dataset\n",
    "    dataset = SimpleCocoDataset()\n",
    "    print(f\"Dataset: {len(dataset)} images, {len(dataset.categories)} classes\")\n",
    "    print(f\"Device: {CONFIG['DEVICE']}\")\n",
    "    \n",
    "    # Run all experiments\n",
    "    all_results = {}\n",
    "    for exp_config in EXPERIMENTS:\n",
    "        results = run_single_experiment(exp_config, dataset)\n",
    "        all_results[exp_config['name']] = results\n",
    "    \n",
    "    # Generate plots\n",
    "    plot_results(results_dir, EXPERIMENTS)\n",
    "    print(f\"\\n✅ All experiments completed! Results saved to '{results_dir}/'\")\n",
    "\n",
    "# Run experiments\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olive-pest-detector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
